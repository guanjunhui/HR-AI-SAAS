# 服务间通信规范

> **版本**: v1.0.0
> **创建日期**: 2026-02-02
> **文档状态**: 开发规范

---

## 一、通信模式概览

### 1.1 通信方式分类

```
┌─────────────────────────────────────────────────────────────┐
│             HR AI SaaS 服务间通信架构                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌────────────────────────┐  ┌────────────────────────┐    │
│  │   同步通信 (RESTful)    │  │   异步通信 (Kafka)      │    │
│  ├────────────────────────┤  ├────────────────────────┤    │
│  │ - OpenFeign            │  │ - 事件发布/订阅         │    │
│  │ - RestTemplate         │  │ - 消息队列             │    │
│  │ - WebClient (可选)     │  │ - 解耦异步处理          │    │
│  │ - 熔断降级 (Resilience4j) │  │ - 最终一致性保证        │    │
│  │ - 负载均衡 (Nacos)     │  │                        │    │
│  └────────────────────────┘  └────────────────────────┘    │
│                                                             │
│  ┌────────────────────────────────────────────────────┐    │
│  │           A2A 通信 (Agent-to-Agent)                 │    │
│  ├────────────────────────────────────────────────────┤    │
│  │ - Agent任务委派                                     │    │
│  │ - 跨服务Agent协作                                   │    │
│  │ - Nacos动态发现                                     │    │
│  │ - 消息持久化 (Kafka)                                │    │
│  └────────────────────────────────────────────────────┘    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 选型原则

| 场景 | 推荐方式 | 理由 |
|------|---------|------|
| 需要即时响应 | OpenFeign (同步) | 简单查询、短流程调用 |
| 可接受延迟 | Kafka (异步) | 解耦、削峰、容错 |
| 跨模块事件通知 | Kafka (异步) | 最终一致性、事件溯源 |
| Agent间协作 | A2A Protocol | Agent任务委派、能力发现 |
| 长流程调用 | Kafka + 回调 | 避免超时、提高吞吐 |

---

## 二、OpenFeign 同步通信规范

### 2.1 依赖配置

```xml
<!-- pom.xml -->
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-openfeign</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-loadbalancer</artifactId>
</dependency>
<dependency>
    <groupId>io.github.resilience4j</groupId>
    <artifactId>resilience4j-spring-boot3</artifactId>
</dependency>
```

### 2.2 启用Feign

```java
@SpringBootApplication
@EnableDiscoveryClient
@EnableFeignClients(basePackages = "com.hrai.*.client")  // 扫描Feign接口
public class Application {
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
```

### 2.3 Feign Client定义规范

#### 2.3.1 标准接口定义

```java
package com.hrai.core.client;

import com.hrai.common.dto.Result;
import com.hrai.org.dto.OrgTreeDTO;
import com.hrai.org.dto.UserDTO;
import org.springframework.cloud.openfeign.FeignClient;
import org.springframework.web.bind.annotation.*;

/**
 * 组织服务 Feign 客户端
 */
@FeignClient(
    name = "hr-org-service",           // 服务名（Nacos注册名）
    path = "/api/org",                 // 服务路径前缀
    fallbackFactory = OrgClientFallbackFactory.class  // 降级工厂
)
public interface OrgClient {

    /**
     * 获取组织树
     */
    @GetMapping("/units/tree")
    Result<OrgTreeDTO> getOrgTree(@RequestParam("tenantId") String tenantId);

    /**
     * 根据ID获取用户
     */
    @GetMapping("/users/{id}")
    Result<UserDTO> getUserById(
        @PathVariable("id") Long id,
        @RequestHeader("X-Tenant-Id") String tenantId
    );

    /**
     * 批量获取用户
     */
    @PostMapping("/users/batch")
    Result<List<UserDTO>> batchGetUsers(
        @RequestBody List<Long> userIds,
        @RequestHeader("X-Tenant-Id") String tenantId
    );

    /**
     * 检查用户权限
     */
    @GetMapping("/permissions/check")
    Result<Boolean> checkPermission(
        @RequestParam("userId") Long userId,
        @RequestParam("permission") String permission,
        @RequestHeader("X-Tenant-Id") String tenantId
    );
}
```

#### 2.3.2 降级处理

```java
package com.hrai.core.client;

import com.hrai.common.dto.Result;
import com.hrai.org.dto.OrgTreeDTO;
import com.hrai.org.dto.UserDTO;
import lombok.extern.slf4j.Slf4j;
import org.springframework.cloud.openfeign.FallbackFactory;
import org.springframework.stereotype.Component;

import java.util.List;

/**
 * OrgClient 降级工厂
 */
@Slf4j
@Component
public class OrgClientFallbackFactory implements FallbackFactory<OrgClient> {

    @Override
    public OrgClient create(Throwable cause) {
        return new OrgClient() {

            @Override
            public Result<OrgTreeDTO> getOrgTree(String tenantId) {
                log.error("获取组织树失败，租户: {}, 原因: {}", tenantId, cause.getMessage());
                // 返回缓存数据或默认数据
                return Result.error("组织服务暂时不可用，请稍后重试");
            }

            @Override
            public Result<UserDTO> getUserById(Long id, String tenantId) {
                log.error("获取用户失败，用户ID: {}, 原因: {}", id, cause.getMessage());
                // 返回缓存的用户信息
                UserDTO cachedUser = getCachedUser(id);
                if (cachedUser != null) {
                    return Result.success(cachedUser);
                }
                return Result.error("用户服务暂时不可用");
            }

            @Override
            public Result<List<UserDTO>> batchGetUsers(List<Long> userIds, String tenantId) {
                log.error("批量获取用户失败，用户数: {}, 原因: {}", userIds.size(), cause.getMessage());
                return Result.error("用户服务暂时不可用");
            }

            @Override
            public Result<Boolean> checkPermission(Long userId, String permission, String tenantId) {
                log.error("权限检查失败，用户: {}, 权限: {}, 原因: {}",
                    userId, permission, cause.getMessage());
                // 降级策略：默认拒绝
                return Result.success(false);
            }

            private UserDTO getCachedUser(Long userId) {
                // 从Redis缓存获取用户信息
                // TODO: 实现缓存逻辑
                return null;
            }
        };
    }
}
```

### 2.4 Feign配置

```yaml
# application.yml
feign:
  client:
    config:
      default:
        connectTimeout: 5000      # 连接超时 5秒
        readTimeout: 10000        # 读取超时 10秒
        loggerLevel: BASIC        # 日志级别: NONE, BASIC, HEADERS, FULL

      # 特定服务配置
      hr-org-service:
        connectTimeout: 3000
        readTimeout: 5000

  # 熔断配置
  circuitbreaker:
    enabled: true
    alphanumeric-ids:
      enabled: true

  # 压缩配置
  compression:
    request:
      enabled: true
      mime-types: text/xml,application/xml,application/json
      min-request-size: 2048
    response:
      enabled: true

# Resilience4j熔断器配置
resilience4j:
  circuitbreaker:
    instances:
      hr-org-service:
        failureRateThreshold: 50              # 失败率阈值 50%
        slowCallRateThreshold: 80             # 慢调用率阈值 80%
        slowCallDurationThreshold: 2s         # 慢调用时长阈值 2秒
        waitDurationInOpenState: 30s          # 开路状态等待时间 30秒
        permittedNumberOfCallsInHalfOpenState: 5  # 半开状态允许调用数 5次
        slidingWindowType: COUNT_BASED        # 滑动窗口类型: COUNT_BASED 或 TIME_BASED
        slidingWindowSize: 10                 # 滑动窗口大小 10次调用
        minimumNumberOfCalls: 5               # 最小调用次数 5次

  # 限流配置
  ratelimiter:
    instances:
      hr-org-service:
        limitForPeriod: 100         # 每个周期限制 100次
        limitRefreshPeriod: 1s      # 刷新周期 1秒
        timeoutDuration: 0.5s       # 超时时间 0.5秒

  # 重试配置
  retry:
    instances:
      hr-org-service:
        maxAttempts: 3              # 最大重试次数 3次
        waitDuration: 500ms         # 重试间隔 500毫秒
        retryExceptions:
          - java.net.SocketTimeoutException
          - java.io.IOException
```

### 2.5 使用示例

```java
@Service
@Slf4j
public class EmployeeService {

    @Autowired
    private OrgClient orgClient;

    public EmployeeDTO getEmployeeWithOrg(Long employeeId) {
        String tenantId = TenantContext.getTenantId();

        // 调用Feign接口
        Result<UserDTO> userResult = orgClient.getUserById(employeeId, tenantId);

        if (!userResult.isSuccess()) {
            log.error("获取用户失败: {}", userResult.getMessage());
            throw new BizException("获取用户信息失败");
        }

        UserDTO user = userResult.getData();

        // 获取组织树
        Result<OrgTreeDTO> orgTreeResult = orgClient.getOrgTree(tenantId);

        EmployeeDTO employee = new EmployeeDTO();
        employee.setUserId(user.getId());
        employee.setUserName(user.getUsername());
        // ... 组装数据

        return employee;
    }
}
```

---

## 三、Kafka 异步通信规范

### 3.1 依赖配置

```xml
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
```

### 3.2 Kafka配置

```yaml
# application.yml
spring:
  kafka:
    bootstrap-servers: ${KAFKA_SERVERS:localhost:9092}

    # 生产者配置
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all                    # 所有副本确认
      retries: 3                   # 重试次数
      batch-size: 16384            # 批量发送大小 16KB
      buffer-memory: 33554432      # 缓冲区大小 32MB
      compression-type: gzip       # 压缩类型

    # 消费者配置
    consumer:
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      group-id: hr-ai-consumer-group
      auto-offset-reset: earliest  # 从最早的消息开始消费
      enable-auto-commit: false    # 手动提交offset
      max-poll-records: 500        # 单次拉取最大记录数
      properties:
        spring.json.trusted.packages: com.hrai.*

    # 监听器配置
    listener:
      ack-mode: manual             # 手动确认
      concurrency: 3               # 并发消费者数量
```

### 3.3 Topic规划

```java
public interface KafkaTopics {
    // 业务事件
    String EMPLOYMENT_EVENT = "employment-event";      // 人事事件
    String OFFER_EVENT = "offer-event";                // Offer事件
    String ONBOARDING_EVENT = "onboarding-event";      // 入职事件
    String ATTENDANCE_EVENT = "attendance-event";      // 考勤事件
    String PAYROLL_EVENT = "payroll-event";            // 薪资事件
    String PERFORMANCE_EVENT = "performance-event";    // 绩效事件

    // AI事件
    String AI_TASK = "ai-task";                        // AI任务
    String AI_INSIGHT_GENERATED = "ai-insight-generated";  // AI洞察生成

    // A2A事件
    String A2A_TASK_DELEGATION = "a2a-task-delegation";    // A2A任务委派
    String A2A_TASK_RESPONSE = "a2a-task-response";        // A2A任务响应

    // 系统事件
    String AUDIT_LOG = "audit-log";                    // 审计日志
    String NOTIFICATION = "notification";              // 通知事件
}
```

### 3.4 事件模型定义

```java
/**
 * 领域事件基类
 */
@Data
public abstract class DomainEvent implements Serializable {
    private String eventId;         // 事件ID（唯一）
    private String eventType;       // 事件类型
    private String tenantId;        // 租户ID
    private Long userId;            // 操作用户ID
    private LocalDateTime timestamp;  // 事件时间
    private String traceId;         // 链路追踪ID
    private Map<String, Object> metadata;  // 元数据

    public DomainEvent() {
        this.eventId = UUID.randomUUID().toString();
        this.timestamp = LocalDateTime.now();
        this.traceId = MDC.get("traceId");
        this.tenantId = TenantContext.getTenantId();
        this.userId = TenantContext.getUserId();
    }
}
```

```java
/**
 * Offer接受事件
 */
@Data
@EqualsAndHashCode(callSuper = true)
public class OfferAcceptedEvent extends DomainEvent {
    private Long offerId;           // Offer ID
    private Long candidateId;       // 候选人ID
    private Long positionId;        // 岗位ID
    private Long orgUnitId;         // 部门ID
    private LocalDate expectedEntryDate;  // 预计入职日期

    public OfferAcceptedEvent() {
        super();
        this.setEventType("offer.accepted");
    }
}
```

### 3.5 事件发布

```java
@Component
@Slf4j
public class EventPublisher {

    @Autowired
    private KafkaTemplate<String, Object> kafkaTemplate;

    /**
     * 发布Offer接受事件
     */
    public void publishOfferAccepted(OfferAcceptedEvent event) {
        try {
            String topic = KafkaTopics.OFFER_EVENT;
            String key = event.getTenantId() + ":" + event.getOfferId();

            ListenableFuture<SendResult<String, Object>> future =
                kafkaTemplate.send(topic, key, event);

            future.addCallback(
                result -> log.info("Offer事件发送成功: topic={}, key={}, offset={}",
                    topic, key, result.getRecordMetadata().offset()),
                ex -> log.error("Offer事件发送失败: topic={}, key={}", topic, key, ex)
            );

        } catch (Exception e) {
            log.error("发布Offer事件异常", e);
            // 可选：写入本地队列或数据库，后续重试
        }
    }

    /**
     * 通用事件发布方法
     */
    public <T extends DomainEvent> void publish(String topic, T event) {
        String key = event.getTenantId() + ":" + event.getEventId();
        kafkaTemplate.send(topic, key, event);
        log.info("事件已发布: topic={}, eventType={}, eventId={}",
            topic, event.getEventType(), event.getEventId());
    }
}
```

### 3.6 事件消费

```java
@Component
@Slf4j
public class OfferEventListener {

    @Autowired
    private OnboardingService onboardingService;

    /**
     * 监听Offer接受事件
     */
    @KafkaListener(
        topics = KafkaTopics.OFFER_EVENT,
        groupId = "onboarding-service-group",
        containerFactory = "kafkaListenerContainerFactory"
    )
    public void handleOfferAccepted(
            @Payload OfferAcceptedEvent event,
            @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
            @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,
            @Header(KafkaHeaders.OFFSET) long offset,
            Acknowledgment acknowledgment) {

        log.info("收到Offer事件: eventId={}, candidateId={}, topic={}, partition={}, offset={}",
            event.getEventId(), event.getCandidateId(), topic, partition, offset);

        try {
            // 设置租户上下文
            TenantContext.setTenantId(event.getTenantId());
            TenantContext.setUserId(event.getUserId());

            // 业务处理：创建入职流程
            onboardingService.createOnboardingProcess(event);

            // 手动确认消费
            acknowledgment.acknowledge();

            log.info("Offer事件处理成功: eventId={}", event.getEventId());

        } catch (Exception e) {
            log.error("Offer事件处理失败: eventId={}", event.getEventId(), e);
            // 异常处理：
            // 1. 记录失败日志到数据库
            // 2. 发送告警
            // 3. 不确认消息（会重新消费）
        } finally {
            TenantContext.clear();
        }
    }
}
```

### 3.7 消息重试与死信队列

```java
@Configuration
public class KafkaErrorHandlerConfig {

    @Bean
    public DefaultErrorHandler errorHandler(
            DeadLetterPublishingRecoverer deadLetterPublishingRecoverer) {

        // 指数退避策略: 1s, 2s, 4s, 8s, 16s, 32s
        BackOff backOff = new ExponentialBackOff(1000, 2.0);

        DefaultErrorHandler errorHandler = new DefaultErrorHandler(
            deadLetterPublishingRecoverer,
            backOff
        );

        // 不重试的异常
        errorHandler.addNotRetryableExceptions(
            BizException.class,
            IllegalArgumentException.class
        );

        return errorHandler;
    }

    @Bean
    public DeadLetterPublishingRecoverer deadLetterPublishingRecoverer(
            KafkaTemplate<String, Object> kafkaTemplate) {

        return new DeadLetterPublishingRecoverer(
            kafkaTemplate,
            (record, ex) -> {
                // 死信队列Topic: 原Topic + .DLT
                String dlqTopic = record.topic() + ".DLT";
                return new TopicPartition(dlqTopic, record.partition());
            }
        );
    }
}
```

---

## 四、通信最佳实践

### 4.1 租户上下文传递

```java
/**
 * Feign拦截器 - 传递租户信息
 */
@Component
public class TenantFeignInterceptor implements RequestInterceptor {

    @Override
    public void apply(RequestTemplate template) {
        String tenantId = TenantContext.getTenantId();
        if (tenantId != null) {
            template.header("X-Tenant-Id", tenantId);
        }

        Long userId = TenantContext.getUserId();
        if (userId != null) {
            template.header("X-User-Id", String.valueOf(userId));
        }

        String traceId = MDC.get("traceId");
        if (traceId != null) {
            template.header("X-Trace-Id", traceId);
        }
    }
}
```

### 4.2 幂等性保证

```java
@Service
@Slf4j
public class IdempotentEventHandler {

    @Autowired
    private RedisTemplate<String, String> redisTemplate;

    private static final String IDEMPOTENT_KEY_PREFIX = "event:consumed:";
    private static final long IDEMPOTENT_TTL = 7 * 24 * 3600; // 7天

    /**
     * 检查事件是否已处理（幂等性检查）
     */
    public boolean isEventProcessed(String eventId) {
        String key = IDEMPOTENT_KEY_PREFIX + eventId;
        Boolean exists = redisTemplate.hasKey(key);
        return Boolean.TRUE.equals(exists);
    }

    /**
     * 标记事件已处理
     */
    public void markEventProcessed(String eventId) {
        String key = IDEMPOTENT_KEY_PREFIX + eventId;
        redisTemplate.opsForValue().set(key, "1", IDEMPOTENT_TTL, TimeUnit.SECONDS);
        log.info("事件已标记为处理: eventId={}", eventId);
    }

    /**
     * 幂等性处理包装
     */
    public <T extends DomainEvent> void handleIdempotent(
            T event,
            Consumer<T> handler) {

        if (isEventProcessed(event.getEventId())) {
            log.warn("事件重复消费，已跳过: eventId={}", event.getEventId());
            return;
        }

        handler.accept(event);

        markEventProcessed(event.getEventId());
    }
}
```

使用示例：

```java
@KafkaListener(topics = KafkaTopics.OFFER_EVENT)
public void handleOfferAccepted(OfferAcceptedEvent event, Acknowledgment ack) {
    idempotentEventHandler.handleIdempotent(event, e -> {
        // 业务处理逻辑
        onboardingService.createOnboardingProcess(e);
    });

    ack.acknowledge();
}
```

### 4.3 分布式事务处理

```java
/**
 * Saga模式事务编排
 */
@Service
@Slf4j
public class OnboardingSagaOrchestrator {

    @Autowired
    private OnboardingService onboardingService;

    @Autowired
    private OrgClient orgClient;

    @Autowired
    private EventPublisher eventPublisher;

    @Transactional
    public void executeOnboardingSaga(OfferAcceptedEvent offerEvent) {
        String sagaId = UUID.randomUUID().toString();

        try {
            // Step 1: 创建入职记录（本地事务）
            Long onboardingId = onboardingService.createOnboarding(offerEvent);

            // Step 2: 创建员工档案（远程调用）
            Result<Long> employeeResult = orgClient.createEmployee(
                buildEmployeeDTO(offerEvent),
                offerEvent.getTenantId()
            );

            if (!employeeResult.isSuccess()) {
                throw new SagaException("创建员工档案失败");
            }

            // Step 3: 发布入职完成事件
            OnboardingCompletedEvent completedEvent = new OnboardingCompletedEvent();
            completedEvent.setOnboardingId(onboardingId);
            completedEvent.setEmployeeId(employeeResult.getData());
            eventPublisher.publish(KafkaTopics.ONBOARDING_EVENT, completedEvent);

            log.info("Saga执行成功: sagaId={}", sagaId);

        } catch (Exception e) {
            log.error("Saga执行失败，开始补偿: sagaId={}", sagaId, e);
            // 补偿操作
            compensate(offerEvent, sagaId);
            throw e;
        }
    }

    private void compensate(OfferAcceptedEvent event, String sagaId) {
        // 补偿逻辑：回滚已执行的操作
        log.info("执行补偿操作: sagaId={}", sagaId);
        // ... 补偿代码
    }
}
```

---

## 五、监控与调试

### 5.1 Feign日志

```yaml
# 开启详细日志
logging:
  level:
    com.hrai.*.client: DEBUG
```

```java
@Configuration
public class FeignLogConfig {

    @Bean
    Logger.Level feignLoggerLevel() {
        return Logger.Level.FULL;  // NONE, BASIC, HEADERS, FULL
    }
}
```

### 5.2 Kafka监控指标

```java
@Configuration
public class KafkaMetricsConfig {

    @Bean
    public MeterRegistryCustomizer<MeterRegistry> kafkaMetrics() {
        return registry -> registry.config().commonTags(
            "application", "hr-ai-saas",
            "service", "hr-core-service"
        );
    }
}
```

访问监控端点：
```bash
curl http://localhost:8082/actuator/metrics/kafka.producer.record.send.total
curl http://localhost:8082/actuator/metrics/kafka.consumer.records.consumed.total
```

---

## 六、常见问题

### 6.1 Feign超时

```
问题：java.net.SocketTimeoutException: Read timed out

解决：
1. 增加超时时间
2. 检查被调用服务性能
3. 考虑改为异步调用
```

### 6.2 Kafka消息堆积

```
问题：消费速度 < 生产速度

解决：
1. 增加消费者并发数
2. 优化消费逻辑性能
3. 分区扩容
4. 检查是否有慢SQL
```

### 6.3 循环依赖

```
问题：服务A调用服务B，服务B又调用服务A

解决：
1. 重新设计服务边界
2. 引入中间服务
3. 使用事件驱动解耦
```

---

> **重要**: 所有服务间调用必须做好熔断降级，避免雪崩效应。建议优先使用异步通信，减少服务间强依赖。
